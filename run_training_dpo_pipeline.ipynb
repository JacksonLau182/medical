{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JacksonLau182/medical/blob/main/run_training_dpo_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "H_p0hXw4YTjO"
      },
      "source": [
        "# Training Pipeline\n",
        "[run_training_dpo_pipeline.ipynb](https://github.com/shibing624/MedicalGPT/blob/main/run_training_dpo_pipeline.ipynb)    | [Open In Colab](https://colab.research.google.com/github/shibing624/MedicalGPT/blob/main/run_training_dpo_pipeline.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "mXEvFrBbYTjQ"
      },
      "source": [
        "# Stage 1: Continue Pretraining\n",
        "\n",
        "第一阶段：PT(Continue PreTraining)增量预训练，在海量领域文本数据上二次预训练GPT模型，以适配领域数据分布\n",
        "\n",
        "注意：\n",
        "1. 此阶段是可选的，如果你没有海量领域文本，可以跳过此阶段，直接进行SFT阶段的有监督微调\n",
        "2. 我实验发现：做领域知识注入，SFT比PT更高效，也可以跳过PT阶段\n",
        "\n",
        "| Stage 1: Continue Pretraining   |  [pretraining.py](https://github.com/shibing624/MedicalGPT/blob/main/pretraining.py) | [run_pt.sh](https://github.com/shibing624/MedicalGPT/blob/main/run_pt.sh)    |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxQn7iYGYTjQ"
      },
      "source": [
        "#### 说明：\n",
        "以下 notebook/colab 代码为了快速验证训练代码可用，我们使用了小size的生成模型和小样本数据集，实际使用时，需要使用更大的模型和数据集，以获得更好的效果。\n",
        "\n",
        "1. 生成模型：使用的是Qwen/Qwen2.5-0.5B\n",
        "2. 数据集：PT阶段使用的是中文天龙八部小说部分文本和英文书籍部分文本，位于`data/pretrain`文件夹"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "r9A0lzM1YTjR"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BkpBpekIYTjR"
      },
      "source": [
        "## 配置运行环境\n",
        "\n",
        "本地执行可注释以下配置环境的命令，colab执行要打开注释，用于配置环境\n",
        "\n",
        "colab建议使用T4 GPU训练，设置方式：`代码执行程序 -> 更改运行时类型 -> 运行时类型：Python3，硬件加速器：GPU，GPU类型：T4 -> 保存`\n",
        "\n",
        "步骤：\n",
        "1. 下载最新代码到本地\n",
        "2. 安装依赖包\n",
        "\n",
        "依赖包如下，保证最新版本：\n",
        "\n",
        "```\n",
        "loguru\n",
        "transformers\n",
        "sentencepiece\n",
        "datasets\n",
        "tensorboard\n",
        "tqdm\n",
        "peft\n",
        "trl\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade --force-reinstall omegaconf==2.4.0.dev3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "id": "Q9lcWQikcLW-",
        "outputId": "8df6ec38-47b7-4ce1-b097-f4b39e4b6e99"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting omegaconf==2.4.0.dev3\n",
            "  Downloading omegaconf-2.4.0.dev3-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting PyYAML>=5.1.0 (from omegaconf==2.4.0.dev3)\n",
            "  Downloading pyyaml-6.0.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (2.4 kB)\n",
            "Downloading omegaconf-2.4.0.dev3-py3-none-any.whl (224 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.4/224.4 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyyaml-6.0.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (807 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m807.9/807.9 kB\u001b[0m \u001b[31m40.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyYAML, omegaconf\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 6.0.3\n",
            "    Uninstalling PyYAML-6.0.3:\n",
            "      Successfully uninstalled PyYAML-6.0.3\n",
            "  Attempting uninstall: omegaconf\n",
            "    Found existing installation: omegaconf 2.3.0\n",
            "    Uninstalling omegaconf-2.3.0:\n",
            "      Successfully uninstalled omegaconf-2.3.0\n",
            "Successfully installed PyYAML-6.0.3 omegaconf-2.4.0.dev3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pydevd_plugins"
                ]
              },
              "id": "301739de743d4350b0c2a0e5e063e3e3"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --force-reinstall antlr4-python3-runtime==4.13.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qvX3oPvJcvCn",
        "outputId": "b89c78b8-3ddb-4107-a843-f8890915c3a4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting antlr4-python3-runtime==4.13.2\n",
            "  Downloading antlr4_python3_runtime-4.13.2-py3-none-any.whl.metadata (304 bytes)\n",
            "Downloading antlr4_python3_runtime-4.13.2-py3-none-any.whl (144 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/144.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.5/144.5 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: antlr4-python3-runtime\n",
            "  Attempting uninstall: antlr4-python3-runtime\n",
            "    Found existing installation: antlr4-python3-runtime 4.9.3\n",
            "    Uninstalling antlr4-python3-runtime-4.9.3:\n",
            "      Successfully uninstalled antlr4-python3-runtime-4.9.3\n",
            "Successfully installed antlr4-python3-runtime-4.13.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip list | grep \"omegaconf\\|antlr4-python3-runtime\\|latex2sympy2-extended\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H3oEp5DJczq3",
        "outputId": "96a92a0f-fe25-41ce-c73d-1ae6513bbc6c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "antlr4-python3-runtime                   4.13.2\n",
            "omegaconf                                2.4.0.dev3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hmtuGlNPYTjR",
        "outputId": "76c24dd2-44a6-4c67-8b59-dbee8d900646"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'MedicalGPT'...\n",
            "remote: Enumerating objects: 98, done.\u001b[K\n",
            "remote: Counting objects: 100% (98/98), done.\u001b[K\n",
            "remote: Compressing objects: 100% (86/86), done.\u001b[K\n",
            "remote: Total 98 (delta 18), reused 58 (delta 7), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (98/98), 8.98 MiB | 13.61 MiB/s, done.\n",
            "Resolving deltas: 100% (18/18), done.\n",
            "/content/MedicalGPT\n",
            "build_domain_tokenizer.py   requirements.txt\n",
            "chatpdf.py                  reward_modeling.py\n",
            "CITATION.cff                \u001b[0m\u001b[01;34mrole_play_data\u001b[0m/\n",
            "_config.yml                 run_dpo.sh\n",
            "CONTRIBUTING.md             run_eval_quantize.sh\n",
            "convert_dataset.py          run_full_sft.sh\n",
            "\u001b[01;34mdata\u001b[0m/                       run_grpo.sh\n",
            "DISCLAIMER                  run_orpo.sh\n",
            "\u001b[01;34mdocs\u001b[0m/                       run_ppo.sh\n",
            "dpo_training.py             run_pt.sh\n",
            "eval_quantize.py            run_quant.sh\n",
            "fastapi_server_demo.py      run_rm.sh\n",
            "gradio_demo.py              run_sft_accelerate.sh\n",
            "grpo_training.py            run_sft.sh\n",
            "inference_multigpu_demo.py  run_training_dpo_pipeline.ipynb\n",
            "inference.py                run_training_ppo_pipeline.ipynb\n",
            "LICENSE                     supervised_finetuning_accelerate.py\n",
            "merge_peft_adapter.py       supervised_finetuning.py\n",
            "merge_tokenizers.py         template.py\n",
            "model_quant.py              validate_jsonl.py\n",
            "openai_api.py               vllm_deployment.sh\n",
            "orpo_training.py            zero1.yaml\n",
            "ppo_training.py             zero2.json\n",
            "pretraining.py              zero2.yaml\n",
            "README_EN.md                zero3.json\n",
            "README.md                   zero3.yaml\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 1)) (1.11.0)\n",
            "Requirement already satisfied: datasets>=2.14.6 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 2)) (4.0.0)\n",
            "Collecting loguru (from -r requirements.txt (line 3))\n",
            "  Downloading loguru-0.7.3-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: peft>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 4)) (0.17.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 5)) (0.2.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 6)) (1.6.1)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 7)) (2.19.0)\n",
            "Requirement already satisfied: tqdm>=4.47.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 8)) (4.67.1)\n",
            "Requirement already satisfied: transformers>=4.49.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 9)) (4.57.1)\n",
            "Collecting trl>=0.15.2 (from -r requirements.txt (line 10))\n",
            "  Downloading trl-0.24.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting latex2sympy2_extended (from -r requirements.txt (line 12))\n",
            "  Downloading latex2sympy2_extended-1.10.2-py3-none-any.whl.metadata (5.3 kB)\n",
            "Collecting math-verify==0.5.2 (from -r requirements.txt (line 13))\n",
            "  Downloading math_verify-0.5.2-py3-none-any.whl.metadata (347 bytes)\n",
            "Collecting latex2sympy2_extended (from -r requirements.txt (line 12))\n",
            "  Downloading latex2sympy2_extended-1.0.6-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.13.2 in /usr/local/lib/python3.12/dist-packages (from latex2sympy2_extended->-r requirements.txt (line 12)) (4.13.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from latex2sympy2_extended->-r requirements.txt (line 12)) (1.13.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from accelerate->-r requirements.txt (line 1)) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from accelerate->-r requirements.txt (line 1)) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate->-r requirements.txt (line 1)) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from accelerate->-r requirements.txt (line 1)) (6.0.3)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate->-r requirements.txt (line 1)) (2.8.0+cu126)\n",
            "Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from accelerate->-r requirements.txt (line 1)) (0.35.3)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate->-r requirements.txt (line 1)) (0.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets>=2.14.6->-r requirements.txt (line 2)) (3.20.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.14.6->-r requirements.txt (line 2)) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.14.6->-r requirements.txt (line 2)) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets>=2.14.6->-r requirements.txt (line 2)) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.14.6->-r requirements.txt (line 2)) (2.32.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets>=2.14.6->-r requirements.txt (line 2)) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.14.6->-r requirements.txt (line 2)) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.14.6->-r requirements.txt (line 2)) (2025.3.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->-r requirements.txt (line 6)) (1.16.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->-r requirements.txt (line 6)) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->-r requirements.txt (line 6)) (3.6.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r requirements.txt (line 7)) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r requirements.txt (line 7)) (1.75.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r requirements.txt (line 7)) (3.9)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r requirements.txt (line 7)) (5.29.5)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r requirements.txt (line 7)) (75.2.0)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r requirements.txt (line 7)) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r requirements.txt (line 7)) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r requirements.txt (line 7)) (3.1.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.49.0->-r requirements.txt (line 9)) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.49.0->-r requirements.txt (line 9)) (0.22.1)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.14.6->-r requirements.txt (line 2)) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions~=4.12 in /usr/local/lib/python3.12/dist-packages (from grpcio>=1.48.2->tensorboard->-r requirements.txt (line 7)) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate->-r requirements.txt (line 1)) (1.1.10)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=2.14.6->-r requirements.txt (line 2)) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=2.14.6->-r requirements.txt (line 2)) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=2.14.6->-r requirements.txt (line 2)) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=2.14.6->-r requirements.txt (line 2)) (2025.10.5)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate->-r requirements.txt (line 1)) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate->-r requirements.txt (line 1)) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate->-r requirements.txt (line 1)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate->-r requirements.txt (line 1)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate->-r requirements.txt (line 1)) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate->-r requirements.txt (line 1)) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate->-r requirements.txt (line 1)) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate->-r requirements.txt (line 1)) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate->-r requirements.txt (line 1)) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate->-r requirements.txt (line 1)) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate->-r requirements.txt (line 1)) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate->-r requirements.txt (line 1)) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate->-r requirements.txt (line 1)) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate->-r requirements.txt (line 1)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate->-r requirements.txt (line 1)) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate->-r requirements.txt (line 1)) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate->-r requirements.txt (line 1)) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->latex2sympy2_extended->-r requirements.txt (line 12)) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard->-r requirements.txt (line 7)) (3.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=2.14.6->-r requirements.txt (line 2)) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=2.14.6->-r requirements.txt (line 2)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=2.14.6->-r requirements.txt (line 2)) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.14.6->-r requirements.txt (line 2)) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.14.6->-r requirements.txt (line 2)) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.14.6->-r requirements.txt (line 2)) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.14.6->-r requirements.txt (line 2)) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.14.6->-r requirements.txt (line 2)) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.14.6->-r requirements.txt (line 2)) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.14.6->-r requirements.txt (line 2)) (1.22.0)\n",
            "Downloading math_verify-0.5.2-py3-none-any.whl (27 kB)\n",
            "Downloading latex2sympy2_extended-1.0.6-py3-none-any.whl (82 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.0/82.0 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading loguru-0.7.3-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trl-0.24.0-py3-none-any.whl (423 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m423.1/423.1 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: loguru, latex2sympy2_extended, math-verify, trl\n",
            "Successfully installed latex2sympy2_extended-1.0.6 loguru-0.7.3 math-verify-0.5.2 trl-0.24.0\n"
          ]
        }
      ],
      "source": [
        "!git clone --depth 1 https://github.com/shibing624/MedicalGPT.git\n",
        "%cd MedicalGPT\n",
        "%ls\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svcanFpHYTjS"
      },
      "source": [
        "## Stage1 咱们开始吧\n",
        "\n",
        "训练步骤如下：\n",
        "\n",
        "1. 确认训练集\n",
        "2. 执行训练脚本\n",
        "\n",
        "训练脚本的执行逻辑如下：\n",
        "1. 导入依赖包\n",
        "2. 设置参数\n",
        "3. 定义各函数并加载训练集\n",
        "4. 加载模型和tokenizer\n",
        "5. 开始训练并评估\n",
        "6. 查看训练结果\n",
        "\n",
        "**以下参数可以根据你的GPU实际情况修改，当前参数是根据Colab的T4单卡GPU（16GB显存）配置的**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jiWI1csFYTjS",
        "outputId": "0e4db5b9-4a86-44a8-c105-3a11fda84727"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: 'MedicalGPT'\n",
            "/content/MedicalGPT\n",
            "build_domain_tokenizer.py   requirements.txt\n",
            "chatpdf.py                  reward_modeling.py\n",
            "CITATION.cff                \u001b[0m\u001b[01;34mrole_play_data\u001b[0m/\n",
            "_config.yml                 run_dpo.sh\n",
            "CONTRIBUTING.md             run_eval_quantize.sh\n",
            "convert_dataset.py          run_full_sft.sh\n",
            "\u001b[01;34mdata\u001b[0m/                       run_grpo.sh\n",
            "DISCLAIMER                  run_orpo.sh\n",
            "\u001b[01;34mdocs\u001b[0m/                       run_ppo.sh\n",
            "dpo_training.py             run_pt.sh\n",
            "eval_quantize.py            run_quant.sh\n",
            "fastapi_server_demo.py      run_rm.sh\n",
            "gradio_demo.py              run_sft_accelerate.sh\n",
            "grpo_training.py            run_sft.sh\n",
            "inference_multigpu_demo.py  run_training_dpo_pipeline.ipynb\n",
            "inference.py                run_training_ppo_pipeline.ipynb\n",
            "LICENSE                     supervised_finetuning_accelerate.py\n",
            "merge_peft_adapter.py       supervised_finetuning.py\n",
            "merge_tokenizers.py         template.py\n",
            "model_quant.py              validate_jsonl.py\n",
            "openai_api.py               vllm_deployment.sh\n",
            "orpo_training.py            zero1.yaml\n",
            "ppo_training.py             zero2.json\n",
            "pretraining.py              zero2.yaml\n",
            "README_EN.md                zero3.json\n",
            "README.md                   zero3.yaml\n",
            "en_article_tail500.txt  fever.txt  tianlongbabu.txt\n"
          ]
        }
      ],
      "source": [
        "%cd MedicalGPT\n",
        "%ls\n",
        "%ls ./data/pretrain/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fv4tujibEsyt",
        "outputId": "4e920b24-20d0-4f9c-9aea-cfc2ace72488"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get update && sudo apt-get install -y build-essential\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-8S4Y0WKftf3",
        "outputId": "880a4689-f8cd-4f47-acf8-1e22258b527f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 https://cli.github.com/packages stable InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com (185.125.190.81)] [Connecting to security.\r                                                                               \rGet:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Hit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:7 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:8 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,817 kB]\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:12 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,389 kB]\n",
            "Hit:13 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,799 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,473 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,594 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [5,865 kB]\n",
            "Get:18 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,288 kB]\n",
            "Fetched 28.6 MB in 3s (9,911 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "build-essential is already the newest version (12.9ubuntu3).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 43 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get update && sudo apt-get install -y build-essential libssl-dev pkg-config\n",
        "!curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y\n",
        "import os\n",
        "os.environ[\"PATH\"] += \":/root/.cargo/bin\"  # 将Rust的bin目录加入环境变量"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tNWovYGQg3b_",
        "outputId": "92626937-58df-4c08-e7ad-a9c2c7ccf160"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 https://cli.github.com/packages stable InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.91.83)] [Connecting to security.ub\r                                                                               \rHit:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "\r0% [Waiting for headers] [Connecting to security.ubuntu.com (185.125.190.81)] [\r                                                                               \rHit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:6 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:8 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "build-essential is already the newest version (12.9ubuntu3).\n",
            "libssl-dev is already the newest version (3.0.2-0ubuntu1.20).\n",
            "The following packages were automatically installed and are no longer required:\n",
            "  libbz2-dev libpkgconf3 libreadline-dev\n",
            "Use 'sudo apt autoremove' to remove them.\n",
            "The following packages will be REMOVED:\n",
            "  pkgconf r-base-dev\n",
            "The following NEW packages will be installed:\n",
            "  pkg-config\n",
            "0 upgraded, 1 newly installed, 2 to remove and 43 not upgraded.\n",
            "Need to get 48.2 kB of archives.\n",
            "After this operation, 10.2 kB disk space will be freed.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 pkg-config amd64 0.29.2-1ubuntu3 [48.2 kB]\n",
            "Fetched 48.2 kB in 0s (163 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "(Reading database ... 126675 files and directories currently installed.)\n",
            "Removing r-base-dev (4.5.1-1.2204.0) ...\n",
            "dpkg: pkgconf: dependency problems, but removing anyway as you requested:\n",
            " libsndfile1-dev:amd64 depends on pkg-config; however:\n",
            "  Package pkg-config is not installed.\n",
            "  Package pkgconf which provides pkg-config is to be removed.\n",
            " libopencv-dev depends on pkg-config; however:\n",
            "  Package pkg-config is not installed.\n",
            "  Package pkgconf which provides pkg-config is to be removed.\n",
            " libmkl-dev:amd64 depends on pkg-config; however:\n",
            "  Package pkg-config is not installed.\n",
            "  Package pkgconf which provides pkg-config is to be removed.\n",
            " libjack-dev depends on pkg-config; however:\n",
            "  Package pkg-config is not installed.\n",
            "  Package pkgconf which provides pkg-config is to be removed.\n",
            " libgphoto2-dev:amd64 depends on pkg-config; however:\n",
            "  Package pkg-config is not installed.\n",
            "  Package pkgconf which provides pkg-config is to be removed.\n",
            " libglib2.0-dev:amd64 depends on pkg-config; however:\n",
            "  Package pkg-config is not installed.\n",
            "  Package pkgconf which provides pkg-config is to be removed.\n",
            " libfontconfig-dev:amd64 depends on pkg-config; however:\n",
            "  Package pkg-config is not installed.\n",
            "  Package pkgconf which provides pkg-config is to be removed.\n",
            "\n",
            "Removing pkgconf (1.8.0-1) ...\n",
            "Removing 'diversion of /usr/bin/pkg-config to /usr/bin/pkg-config.real by pkgconf'\n",
            "Removing 'diversion of /usr/share/aclocal/pkg.m4 to /usr/share/aclocal/pkg.real.m4 by pkgconf'\n",
            "Removing 'diversion of /usr/share/man/man1/pkg-config.1.gz to /usr/share/man/man1/pkg-config.real.1.gz by pkgconf'\n",
            "Removing 'diversion of /usr/share/pkg-config-crosswrapper to /usr/share/pkg-config-crosswrapper.real by pkgconf'\n",
            "Selecting previously unselected package pkg-config.\n",
            "(Reading database ... 126651 files and directories currently installed.)\n",
            "Preparing to unpack .../pkg-config_0.29.2-1ubuntu3_amd64.deb ...\n",
            "Unpacking pkg-config (0.29.2-1ubuntu3) ...\n",
            "Setting up pkg-config (0.29.2-1ubuntu3) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "\u001b[1minfo:\u001b[0m downloading installer\n",
            "\u001b[0m\u001b[1minfo: \u001b[0mprofile set to 'default'\n",
            "\u001b[0m\u001b[1minfo: \u001b[0mdefault host triple is x86_64-unknown-linux-gnu\n",
            "\u001b[0m\u001b[1minfo: \u001b[0msyncing channel updates for 'stable-x86_64-unknown-linux-gnu'\n",
            "\u001b[0m\u001b[1minfo: \u001b[0mlatest update on 2025-09-18, rust version 1.90.0 (1159e78c4 2025-09-14)\n",
            "\u001b[0m\u001b[1minfo: \u001b[0mdownloading component 'cargo'\n",
            "\u001b[0m\u001b[1minfo: \u001b[0mdownloading component 'clippy'\n",
            "\u001b[0m\u001b[1minfo: \u001b[0mdownloading component 'rust-docs'\n",
            "\u001b[0m\u001b[1minfo: \u001b[0mdownloading component 'rust-std'\n",
            "\u001b[0m\u001b[1minfo: \u001b[0mdownloading component 'rustc'\n",
            "\u001b[0m\u001b[1minfo: \u001b[0mdownloading component 'rustfmt'\n",
            "\u001b[0m\u001b[1minfo: \u001b[0minstalling component 'cargo'\n",
            "\u001b[0m\u001b[1minfo: \u001b[0minstalling component 'clippy'\n",
            "\u001b[0m\u001b[1minfo: \u001b[0minstalling component 'rust-docs'\n",
            " 20.5 MiB /  20.5 MiB (100 %)   1.9 MiB/s in  8s\n",
            "\u001b[0m\u001b[1minfo: \u001b[0minstalling component 'rust-std'\n",
            " 27.8 MiB /  27.8 MiB (100 %)  10.3 MiB/s in  3s\n",
            "\u001b[0m\u001b[1minfo: \u001b[0minstalling component 'rustc'\n",
            " 78.7 MiB /  78.7 MiB (100 %)  10.4 MiB/s in  8s\n",
            "\u001b[0m\u001b[1minfo: \u001b[0minstalling component 'rustfmt'\n",
            "\u001b[0m\u001b[1minfo: \u001b[0mdefault toolchain set to 'stable-x86_64-unknown-linux-gnu'\n",
            "\n",
            "  \u001b[0m\u001b[1m\u001b[0m\u001b[1m\u001b[32mstable-x86_64-unknown-linux-gnu installed\u001b[0m - rustc 1.90.0 (1159e78c4 2025-09-14)\n",
            "\n",
            "\u001b[0m\u001b[1m\n",
            "Rust is installed now. Great!\n",
            "\u001b[0m\n",
            "To get started you may need to restart your current shell.\n",
            "This would reload your \u001b[0m\u001b[1mPATH\u001b[0m environment variable to include\n",
            "Cargo's bin directory ($HOME/.cargo/bin).\n",
            "\n",
            "To configure your current shell, you need to source\n",
            "the corresponding \u001b[0m\u001b[1menv\u001b[0m file under $HOME/.cargo.\n",
            "\n",
            "This is usually done by running one of the following (note the leading DOT):\n",
            ". \"$HOME/.cargo/env\"            # For sh/bash/zsh/ash/dash/pdksh\n",
            "source \"$HOME/.cargo/env.fish\"  # For fish\n",
            "source $\"($nu.home-path)/.cargo/env.nu\"  # For nushell\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --force-reinstall tokenizers==0.13.3\n",
        "!pip install --force-reinstall transformers==4.28.1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bSe_FZC2gbNv",
        "outputId": "a7c7e132-ee63-468c-a459-9ea62729c87d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tokenizers==0.13.3\n",
            "  Using cached tokenizers-0.13.3.tar.gz (314 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: tokenizers\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mBuilding wheel for tokenizers \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for tokenizers (pyproject.toml) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[31m  ERROR: Failed building wheel for tokenizers\u001b[0m\u001b[31m\n",
            "\u001b[0mFailed to build tokenizers\n",
            "\u001b[31mERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (tokenizers)\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting transformers==4.28.1\n",
            "  Using cached transformers-4.28.1-py3-none-any.whl.metadata (109 kB)\n",
            "Collecting filelock (from transformers==4.28.1)\n",
            "  Using cached filelock-3.20.0-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0 (from transformers==4.28.1)\n",
            "  Using cached huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting numpy>=1.17 (from transformers==4.28.1)\n",
            "  Using cached numpy-2.3.4-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
            "Collecting packaging>=20.0 (from transformers==4.28.1)\n",
            "  Using cached packaging-25.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting pyyaml>=5.1 (from transformers==4.28.1)\n",
            "  Using cached pyyaml-6.0.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (2.4 kB)\n",
            "Collecting regex!=2019.12.17 (from transformers==4.28.1)\n",
            "  Using cached regex-2025.10.23-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
            "Collecting requests (from transformers==4.28.1)\n",
            "  Using cached requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.28.1)\n",
            "  Using cached tokenizers-0.13.3.tar.gz (314 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting tqdm>=4.27 (from transformers==4.28.1)\n",
            "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
            "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.11.0->transformers==4.28.1)\n",
            "  Using cached fsspec-2025.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting typing-extensions>=3.7.4.3 (from huggingface-hub<1.0,>=0.11.0->transformers==4.28.1)\n",
            "  Using cached typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub<1.0,>=0.11.0->transformers==4.28.1)\n",
            "  Using cached hf_xet-1.1.10-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.7 kB)\n",
            "Collecting charset_normalizer<4,>=2 (from requests->transformers==4.28.1)\n",
            "  Using cached charset_normalizer-3.4.4-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (37 kB)\n",
            "Collecting idna<4,>=2.5 (from requests->transformers==4.28.1)\n",
            "  Using cached idna-3.11-py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting urllib3<3,>=1.21.1 (from requests->transformers==4.28.1)\n",
            "  Using cached urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting certifi>=2017.4.17 (from requests->transformers==4.28.1)\n",
            "  Using cached certifi-2025.10.5-py3-none-any.whl.metadata (2.5 kB)\n",
            "Using cached transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
            "Using cached huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
            "Using cached numpy-2.3.4-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.6 MB)\n",
            "Using cached packaging-25.0-py3-none-any.whl (66 kB)\n",
            "Using cached pyyaml-6.0.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (807 kB)\n",
            "Using cached regex-2025.10.23-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (803 kB)\n",
            "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
            "Using cached filelock-3.20.0-py3-none-any.whl (16 kB)\n",
            "Using cached requests-2.32.5-py3-none-any.whl (64 kB)\n",
            "Using cached certifi-2025.10.5-py3-none-any.whl (163 kB)\n",
            "Using cached charset_normalizer-3.4.4-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (153 kB)\n",
            "Using cached fsspec-2025.9.0-py3-none-any.whl (199 kB)\n",
            "Using cached hf_xet-1.1.10-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
            "Using cached idna-3.11-py3-none-any.whl (71 kB)\n",
            "Using cached typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
            "Using cached urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
            "Building wheels for collected packages: tokenizers\n",
            "  Building wheel for tokenizers (pyproject.toml) ... \u001b[?25l\u001b[?25hcanceled\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/base_command.py\", line 179, in exc_logging_wrapper\n",
            "    status = run_func(*args)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/req_command.py\", line 67, in wrapper\n",
            "    return func(self, options, args)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/commands/install.py\", line 423, in run\n",
            "    _, build_failures = build(\n",
            "                        ^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/wheel_builder.py\", line 319, in build\n",
            "    wheel_file = _build_one(\n",
            "                 ^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/wheel_builder.py\", line 193, in _build_one\n",
            "    wheel_path = _build_one_inside_env(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/wheel_builder.py\", line 233, in _build_one_inside_env\n",
            "    wheel_path = build_wheel_pep517(\n",
            "                 ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/operations/build/wheel.py\", line 30, in build_wheel_pep517\n",
            "    wheel_name = backend.build_wheel(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/utils/misc.py\", line 682, in build_wheel\n",
            "    return super().build_wheel(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_vendor/pyproject_hooks/_impl.py\", line 209, in build_wheel\n",
            "    return self._call_hook('build_wheel', {\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_vendor/pyproject_hooks/_impl.py\", line 311, in _call_hook\n",
            "    self._subprocess_runner(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/utils/subprocess.py\", line 237, in runner\n",
            "    call_subprocess(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/utils/subprocess.py\", line 151, in call_subprocess\n",
            "    line: str = proc.stdout.readline()\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/pip3\", line 10, in <module>\n",
            "    sys.exit(main())\n",
            "             ^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/main.py\", line 80, in main\n",
            "    return command.main(cmd_args)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/base_command.py\", line 100, in main\n",
            "    return self._main(args)\n",
            "           ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/base_command.py\", line 232, in _main\n",
            "    return run(options, args)\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/base_command.py\", line 215, in exc_logging_wrapper\n",
            "    logger.critical(\"Operation cancelled by user\")\n",
            "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1576, in critical\n",
            "    def critical(self, msg, *args, **kwargs):\n",
            "\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --force-reinstall transformers==4.57.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "xQKjIihWjO_G",
        "outputId": "31c58f15-2f2f-4598-eaa5-cedd8b387979"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==4.57.1\n",
            "  Downloading transformers-4.57.1-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/44.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting filelock (from transformers==4.57.1)\n",
            "  Using cached filelock-3.20.0-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers==4.57.1)\n",
            "  Using cached huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting numpy>=1.17 (from transformers==4.57.1)\n",
            "  Using cached numpy-2.3.4-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
            "Collecting packaging>=20.0 (from transformers==4.57.1)\n",
            "  Using cached packaging-25.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting pyyaml>=5.1 (from transformers==4.57.1)\n",
            "  Using cached pyyaml-6.0.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (2.4 kB)\n",
            "Collecting regex!=2019.12.17 (from transformers==4.57.1)\n",
            "  Using cached regex-2025.10.23-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
            "Collecting requests (from transformers==4.57.1)\n",
            "  Using cached requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers==4.57.1)\n",
            "  Downloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Collecting safetensors>=0.4.3 (from transformers==4.57.1)\n",
            "  Downloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Collecting tqdm>=4.27 (from transformers==4.57.1)\n",
            "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
            "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.34.0->transformers==4.57.1)\n",
            "  Using cached fsspec-2025.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting typing-extensions>=3.7.4.3 (from huggingface-hub<1.0,>=0.34.0->transformers==4.57.1)\n",
            "  Using cached typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub<1.0,>=0.34.0->transformers==4.57.1)\n",
            "  Using cached hf_xet-1.1.10-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.7 kB)\n",
            "Collecting charset_normalizer<4,>=2 (from requests->transformers==4.57.1)\n",
            "  Using cached charset_normalizer-3.4.4-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (37 kB)\n",
            "Collecting idna<4,>=2.5 (from requests->transformers==4.57.1)\n",
            "  Using cached idna-3.11-py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting urllib3<3,>=1.21.1 (from requests->transformers==4.57.1)\n",
            "  Using cached urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting certifi>=2017.4.17 (from requests->transformers==4.57.1)\n",
            "  Using cached certifi-2025.10.5-py3-none-any.whl.metadata (2.5 kB)\n",
            "Downloading transformers-4.57.1-py3-none-any.whl (12.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m130.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
            "Using cached numpy-2.3.4-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.6 MB)\n",
            "Using cached packaging-25.0-py3-none-any.whl (66 kB)\n",
            "Using cached pyyaml-6.0.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (807 kB)\n",
            "Using cached regex-2025.10.23-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (803 kB)\n",
            "Downloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (485 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.8/485.8 kB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m125.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
            "Using cached filelock-3.20.0-py3-none-any.whl (16 kB)\n",
            "Using cached requests-2.32.5-py3-none-any.whl (64 kB)\n",
            "Using cached certifi-2025.10.5-py3-none-any.whl (163 kB)\n",
            "Using cached charset_normalizer-3.4.4-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (153 kB)\n",
            "Using cached fsspec-2025.9.0-py3-none-any.whl (199 kB)\n",
            "Using cached hf_xet-1.1.10-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
            "Using cached idna-3.11-py3-none-any.whl (71 kB)\n",
            "Using cached typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
            "Using cached urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
            "Installing collected packages: urllib3, typing-extensions, tqdm, safetensors, regex, pyyaml, packaging, numpy, idna, hf-xet, fsspec, filelock, charset_normalizer, certifi, requests, huggingface-hub, tokenizers, transformers\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.5.0\n",
            "    Uninstalling urllib3-2.5.0:\n",
            "      Successfully uninstalled urllib3-2.5.0\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.15.0\n",
            "    Uninstalling typing_extensions-4.15.0:\n",
            "      Successfully uninstalled typing_extensions-4.15.0\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.67.1\n",
            "    Uninstalling tqdm-4.67.1:\n",
            "      Successfully uninstalled tqdm-4.67.1\n",
            "  Attempting uninstall: safetensors\n",
            "    Found existing installation: safetensors 0.6.2\n",
            "    Uninstalling safetensors-0.6.2:\n",
            "      Successfully uninstalled safetensors-0.6.2\n",
            "  Attempting uninstall: regex\n",
            "    Found existing installation: regex 2024.11.6\n",
            "    Uninstalling regex-2024.11.6:\n",
            "      Successfully uninstalled regex-2024.11.6\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 6.0.3\n",
            "    Uninstalling PyYAML-6.0.3:\n",
            "      Successfully uninstalled PyYAML-6.0.3\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 25.0\n",
            "    Uninstalling packaging-25.0:\n",
            "      Successfully uninstalled packaging-25.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.11\n",
            "    Uninstalling idna-3.11:\n",
            "      Successfully uninstalled idna-3.11\n",
            "  Attempting uninstall: hf-xet\n",
            "    Found existing installation: hf-xet 1.1.10\n",
            "    Uninstalling hf-xet-1.1.10:\n",
            "      Successfully uninstalled hf-xet-1.1.10\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.0\n",
            "    Uninstalling fsspec-2025.3.0:\n",
            "      Successfully uninstalled fsspec-2025.3.0\n",
            "  Attempting uninstall: filelock\n",
            "    Found existing installation: filelock 3.20.0\n",
            "    Uninstalling filelock-3.20.0:\n",
            "      Successfully uninstalled filelock-3.20.0\n",
            "  Attempting uninstall: charset_normalizer\n",
            "    Found existing installation: charset-normalizer 3.4.4\n",
            "    Uninstalling charset-normalizer-3.4.4:\n",
            "      Successfully uninstalled charset-normalizer-3.4.4\n",
            "  Attempting uninstall: certifi\n",
            "    Found existing installation: certifi 2025.10.5\n",
            "    Uninstalling certifi-2025.10.5:\n",
            "      Successfully uninstalled certifi-2025.10.5\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.32.4\n",
            "    Uninstalling requests-2.32.4:\n",
            "      Successfully uninstalled requests-2.32.4\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.35.3\n",
            "    Uninstalling huggingface-hub-0.35.3:\n",
            "      Successfully uninstalled huggingface-hub-0.35.3\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.22.1\n",
            "    Uninstalling tokenizers-0.22.1:\n",
            "      Successfully uninstalled tokenizers-0.22.1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.57.1\n",
            "    Uninstalling transformers-4.57.1:\n",
            "      Successfully uninstalled transformers-4.57.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.4 which is incompatible.\n",
            "datasets 4.0.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.9.0 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.3.4 which is incompatible.\n",
            "tensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.3.4 which is incompatible.\n",
            "cupy-cuda12x 13.3.0 requires numpy<2.3,>=1.22, but you have numpy 2.3.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.4 which is incompatible.\n",
            "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.9.0 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed certifi-2025.10.5 charset_normalizer-3.4.4 filelock-3.20.0 fsspec-2025.9.0 hf-xet-1.1.10 huggingface-hub-0.36.0 idna-3.11 numpy-2.3.4 packaging-25.0 pyyaml-6.0.3 regex-2025.10.23 requests-2.32.5 safetensors-0.6.2 tokenizers-0.22.1 tqdm-4.67.1 transformers-4.57.1 typing-extensions-4.15.0 urllib3-2.5.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "certifi",
                  "numpy",
                  "packaging"
                ]
              },
              "id": "394112e402e34899bc223a94a3582dc2"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pcN82dTHYTjS",
        "outputId": "c1da72e3-7e6b-4ffe-8894-edd17d8a4d0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-10-24 09:34:25.166003: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1761298465.187437   23076 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1761298465.193476   23076 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1761298465.210765   23076 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761298465.210789   23076 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761298465.210793   23076 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761298465.210797   23076 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-10-24 09:34:25.215672: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[32m2025-10-24 09:34:29.273\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m362\u001b[0m - \u001b[1mModel args: ModelArguments(model_name_or_path='Qwen/Qwen2.5-0.5B', tokenizer_name_or_path=None, load_in_8bit=False, load_in_4bit=False, cache_dir=None, model_revision='main', hf_hub_token=None, use_fast_tokenizer=False, torch_dtype='bfloat16', device_map='auto', trust_remote_code=True)\u001b[0m\n",
            "\u001b[32m2025-10-24 09:34:29.273\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m363\u001b[0m - \u001b[1mData args: DataArguments(dataset_name=None, dataset_config_name=None, train_file_dir='./data/pretrain', validation_file_dir='./data/pretrain', max_train_samples=20000, max_eval_samples=10, streaming=False, block_size=128, overwrite_cache=False, validation_split_percentage=1, preprocessing_num_workers=1, keep_linebreaks=True)\u001b[0m\n",
            "\u001b[32m2025-10-24 09:34:29.274\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m364\u001b[0m - \u001b[1mTraining args: Seq2SeqTrainingArguments(\n",
            "_n_gpu=1,\n",
            "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "average_tokens_across_devices=True,\n",
            "batch_eval_metrics=False,\n",
            "bf16=True,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_persistent_workers=False,\n",
            "dataloader_pin_memory=True,\n",
            "dataloader_prefetch_factor=None,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=False,\n",
            "ddp_timeout=30000,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_do_concat_batches=True,\n",
            "eval_on_start=False,\n",
            "eval_steps=50,\n",
            "eval_strategy=IntervalStrategy.STEPS,\n",
            "eval_use_gather_object=False,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "generation_config=None,\n",
            "generation_max_length=None,\n",
            "generation_num_beams=None,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=True,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=True,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=None,\n",
            "hub_revision=None,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_for_metrics=[],\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=no,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=0.0002,\n",
            "length_column_name=length,\n",
            "liger_kernel_config=None,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=outputs-pt-v1/runs/Oct24_09-34-29_ea78e3f0f486,\n",
            "logging_first_step=True,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=10,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=1.0,\n",
            "optim=OptimizerNames.ADAMW_TORCH_FUSED,\n",
            "optim_args=None,\n",
            "optim_target_modules=None,\n",
            "output_dir=outputs-pt-v1,\n",
            "overwrite_output_dir=True,\n",
            "parallelism_config=None,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=3,\n",
            "per_device_train_batch_size=3,\n",
            "predict_with_generate=False,\n",
            "prediction_loss_only=False,\n",
            "project=huggingface,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "restore_callback_states_from_checkpoint=False,\n",
            "resume_from_checkpoint=None,\n",
            "run_name=None,\n",
            "save_on_each_node=False,\n",
            "save_only_model=False,\n",
            "save_safetensors=True,\n",
            "save_steps=50,\n",
            "save_strategy=SaveStrategy.STEPS,\n",
            "save_total_limit=3,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "sortish_sampler=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torch_empty_cache_steps=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "trackio_space_id=trackio,\n",
            "use_cpu=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_liger_kernel=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.05,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.01,\n",
            ")\u001b[0m\n",
            "\u001b[32m2025-10-24 09:34:29.274\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m365\u001b[0m - \u001b[1mScript args: ScriptArguments(use_peft=True, target_modules='all', lora_rank=8, lora_dropout=0.05, lora_alpha=16.0, modules_to_save=None, peft_path=None, qlora=False)\u001b[0m\n",
            "\u001b[32m2025-10-24 09:34:29.274\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m366\u001b[0m - \u001b[1mProcess rank: 0, device: cuda:0, n_gpu: 1 distributed training: True, 16-bits training: False\u001b[0m\n",
            "tokenizer_config.json: 7.23kB [00:00, 29.1MB/s]\n",
            "vocab.json: 2.78MB [00:00, 61.1MB/s]\n",
            "merges.txt: 1.67MB [00:00, 138MB/s]\n",
            "tokenizer.json: 7.03MB [00:00, 135MB/s]\n",
            "\u001b[32m2025-10-24 09:34:30.776\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m474\u001b[0m - \u001b[1mtrain files: ['./data/pretrain/fever.txt', './data/pretrain/tianlongbabu.txt', './data/pretrain/en_article_tail500.txt']\u001b[0m\n",
            "\u001b[32m2025-10-24 09:34:30.776\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m484\u001b[0m - \u001b[1meval files: ['./data/pretrain/fever.txt', './data/pretrain/tianlongbabu.txt', './data/pretrain/en_article_tail500.txt']\u001b[0m\n",
            "Generating train split: 3876 examples [00:00, 112654.16 examples/s]\n",
            "Generating validation split: 3876 examples [00:00, 250379.21 examples/s]\n",
            "\u001b[32m2025-10-24 09:34:31.280\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m516\u001b[0m - \u001b[1mRaw datasets: DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['text'],\n",
            "        num_rows: 3876\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['text'],\n",
            "        num_rows: 3876\n",
            "    })\n",
            "})\u001b[0m\n",
            "Running tokenizer on dataset: 100% 3876/3876 [00:10<00:00, 376.01 examples/s]\n",
            "Running tokenizer on dataset: 100% 3876/3876 [00:16<00:00, 229.36 examples/s]\n",
            "Grouping texts in chunks of 128: 100% 3876/3876 [00:00<00:00, 9243.69 examples/s]\n",
            "Grouping texts in chunks of 128: 100% 3876/3876 [00:00<00:00, 9426.38 examples/s]\n",
            "\u001b[32m2025-10-24 09:35:04.384\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m579\u001b[0m - \u001b[34m\u001b[1mNum train_samples: 2503\u001b[0m\n",
            "\u001b[32m2025-10-24 09:35:04.385\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m580\u001b[0m - \u001b[34m\u001b[1mTokenized training example:\u001b[0m\n",
            "\u001b[32m2025-10-24 09:35:04.385\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m581\u001b[0m - \u001b[34m\u001b[1m第一章论\n",
            "传染病是指由病原微生物，如朊粒、病毒、衣原体、立克次体、支原体（mycoplasma)细菌真菌、螺旋体和寄生虫，如原虫、蠕虫、医学昆虫感染人体后产生的有传染性、在一定条件下可造成流行的疾病。感染性疾病是指由病原体感染所致的疾病，包括传染病和非传染性感染性疾病。\n",
            "传染病学是一门研究各种传染病在人体内外发生、发展、传播、诊断、治疗和预防规律的学科。重点研究各种传染病的发病机制、临床表现、\u001b[0m\n",
            "\u001b[32m2025-10-24 09:35:04.387\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m593\u001b[0m - \u001b[34m\u001b[1mNum eval_samples: 10\u001b[0m\n",
            "\u001b[32m2025-10-24 09:35:04.387\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m594\u001b[0m - \u001b[34m\u001b[1mTokenized eval example:\u001b[0m\n",
            "\u001b[32m2025-10-24 09:35:04.387\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m595\u001b[0m - \u001b[34m\u001b[1m第一章论\n",
            "传染病是指由病原微生物，如朊粒、病毒、衣原体、立克次体、支原体（mycoplasma)细菌真菌、螺旋体和寄生虫，如原虫、蠕虫、医学昆虫感染人体后产生的有传染性、在一定条件下可造成流行的疾病。感染性疾病是指由病原体感染所致的疾病，包括传染病和非传染性感染性疾病。\n",
            "传染病学是一门研究各种传染病在人体内外发生、发展、传播、诊断、治疗和预防规律的学科。重点研究各种传染病的发病机制、临床表现、\u001b[0m\n",
            "config.json: 100% 681/681 [00:00<00:00, 6.18MB/s]\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "model.safetensors: 100% 988M/988M [00:24<00:00, 40.5MB/s]\n",
            "generation_config.json: 100% 138/138 [00:00<00:00, 1.25MB/s]\n",
            "\u001b[32m2025-10-24 09:35:30.219\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m654\u001b[0m - \u001b[1mFine-tuning method: LoRA(PEFT)\u001b[0m\n",
            "\u001b[32m2025-10-24 09:35:30.219\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m659\u001b[0m - \u001b[1mInit new peft model\u001b[0m\n",
            "\u001b[32m2025-10-24 09:35:30.220\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m672\u001b[0m - \u001b[1mPeft target_modules: ['down_proj', 'gate_proj', 'k_proj', 'o_proj', 'q_proj', 'up_proj', 'v_proj']\u001b[0m\n",
            "\u001b[32m2025-10-24 09:35:30.220\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m673\u001b[0m - \u001b[1mPeft lora_rank: 8\u001b[0m\n",
            "trainable params: 4,399,104 || all params: 498,431,872 || trainable%: 0.8826\n",
            "/content/MedicalGPT/pretraining.py:703: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `SavePeftModelTrainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = SavePeftModelTrainer(\n",
            "The model is already on multiple devices. Skipping the move to device specified in `args`.\n",
            "\u001b[32m2025-10-24 09:35:30.783\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m718\u001b[0m - \u001b[1m*** Train ***\u001b[0m\n",
            "\u001b[32m2025-10-24 09:35:31.749\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m719\u001b[0m - \u001b[34m\u001b[1mTrain dataloader example: {'input_ids': tensor([[118458,   5373, 101631,  99389,   5373, 100819, 116154, 101202,   5373,\n",
            "         108246,  49238,  99178,  81217,  99918, 106939, 102243, 117489,  49567,\n",
            "           1773, 101067, 102249, 101364,  99361, 103949,  90395,  28291,  99769,\n",
            "          99726,  95355, 104052, 102008,   3837, 104110, 100347,   3837, 113130,\n",
            "         111142, 102062, 102487,  54542,   8997,     21,     13, 115669,   9370,\n",
            "         102681,    198, 101364,  33071,  99931,  99252,   5122,  99285, 102835,\n",
            "         104579,  24968, 100662, 111832,  99364, 118944,   3837, 111378, 113492,\n",
            "         100489,  27773, 100443,   5373, 101176, 109958,  99617,  77419,  49567,\n",
            "         106555, 113217, 107368,   9370, 101082, 101940, 101810,   9370, 100394,\n",
            "          33108, 104460,  24968, 106332, 101910, 100489,  27773, 100443,  57191,\n",
            "          99932,  99918, 114357, 104465, 102526, 100859,   3837, 100667, 107727,\n",
            "         100859,  31843,  95312, 101810, 101252,   3837,  32555, 100859,  31843,\n",
            "             79,     39, 100662,  18493,     20,     93,     21,   9370,  99835,\n",
            "          99918,  99719,   3837, 101940, 101810,   9370, 101894,  33108, 104460,\n",
            "           3837, 100366],\n",
            "        [102067,  32945, 102903, 119516,  99315, 100323,  26288,  48738,   3837,\n",
            "         104094,  36987,  43288, 108634, 109380, 110564, 100090,  35946,  17714,\n",
            "          99235, 100186,  32945,  37474,  36556, 112862,  44793,  36987,  99425,\n",
            "         102430,   3837,  35946, 108610,  99590, 101438,   3837,  56568,  14053,\n",
            "          56568,  53222,  34187,  35946,  99406,  99261,  32945,    198, 101141,\n",
            "          99425, 102430,  32664,  37474,  36556, 112862, 101747,  36589,  90395,\n",
            "          16530,  62112,  99859,  49828, 101526,   7948,  68536, 109607,  99773,\n",
            "          34187,   3837,  49187, 104318,  49828, 101073,  39374,  99508,   3837,\n",
            "          28641,  13343,  63109,  64272,   3837,  44793,  36987,  56568, 100672,\n",
            "          14053, 100672, 108610,  99590, 101438,  81264,  37474,  36556, 112862,\n",
            "          44793,  36987,  20412,   3837,  20412,  75758,  75061, 105700, 101255,\n",
            "          39426,  44793,  36987,  99235, 103856,   3837,  43288,  99393,  63109,\n",
            "         115670, 100363,   3837,  56568,  99518, 100698,  49828,   9370,  11319,\n",
            "         102413,  40820, 100660,   3837, 104335,  99314, 100003,  75758,    198,\n",
            "         102903, 119516],\n",
            "        [101978,  39907, 101098,   5373, 105521, 100136,  65278,  62945,   8997,\n",
            "           9909,  63703,   7552, 106693,  47764, 101071,    198, 102749,  24300,\n",
            "         101924,     55,  43268,  30440, 100347,  99632,  18493, 111464,  99762,\n",
            "         111684,   8997,  10904, 115669,  10958,    198,  30440, 115668, 107368,\n",
            "          33071,  17447, 115610, 101304,   5373,  99180,  35551, 100439,  57191,\n",
            "          45356,  99180,  35551, 100439,   3837, 107368,  33071, 102749,   3837,\n",
            "         109470,  33071, 100242,  99316,   3837, 109470,  33071,  63109, 101408,\n",
            "         100439,  57191,  99931, 101364, 105349,  74040, 118458,  49567,   1773,\n",
            "          90919,  99931,  89481, 101364, 105349,  74040,    693,   9011, 118458,\n",
            "           3837,  38176, 101304, 100043,  24300,  33108, 100916,  24300, 112103,\n",
            "         101320,   9370, 101924, 112506,   5373, 118439,  31838, 106806, 115669,\n",
            "           3837, 100674, 106122,   3837, 100140,  87267,  57218, 107272,  99727,\n",
            "          64643, 101573,  99463, 101063,   8997,  10904, 105262,  57218, 115653,\n",
            "         105262, 144083,   9909,  14777,   7552, 105262,  39165,  38342, 100347,\n",
            "         112103, 102529]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0'), 'labels': tensor([[118458,   5373, 101631,  99389,   5373, 100819, 116154, 101202,   5373,\n",
            "         108246,  49238,  99178,  81217,  99918, 106939, 102243, 117489,  49567,\n",
            "           1773, 101067, 102249, 101364,  99361, 103949,  90395,  28291,  99769,\n",
            "          99726,  95355, 104052, 102008,   3837, 104110, 100347,   3837, 113130,\n",
            "         111142, 102062, 102487,  54542,   8997,     21,     13, 115669,   9370,\n",
            "         102681,    198, 101364,  33071,  99931,  99252,   5122,  99285, 102835,\n",
            "         104579,  24968, 100662, 111832,  99364, 118944,   3837, 111378, 113492,\n",
            "         100489,  27773, 100443,   5373, 101176, 109958,  99617,  77419,  49567,\n",
            "         106555, 113217, 107368,   9370, 101082, 101940, 101810,   9370, 100394,\n",
            "          33108, 104460,  24968, 106332, 101910, 100489,  27773, 100443,  57191,\n",
            "          99932,  99918, 114357, 104465, 102526, 100859,   3837, 100667, 107727,\n",
            "         100859,  31843,  95312, 101810, 101252,   3837,  32555, 100859,  31843,\n",
            "             79,     39, 100662,  18493,     20,     93,     21,   9370,  99835,\n",
            "          99918,  99719,   3837, 101940, 101810,   9370, 101894,  33108, 104460,\n",
            "           3837, 100366],\n",
            "        [102067,  32945, 102903, 119516,  99315, 100323,  26288,  48738,   3837,\n",
            "         104094,  36987,  43288, 108634, 109380, 110564, 100090,  35946,  17714,\n",
            "          99235, 100186,  32945,  37474,  36556, 112862,  44793,  36987,  99425,\n",
            "         102430,   3837,  35946, 108610,  99590, 101438,   3837,  56568,  14053,\n",
            "          56568,  53222,  34187,  35946,  99406,  99261,  32945,    198, 101141,\n",
            "          99425, 102430,  32664,  37474,  36556, 112862, 101747,  36589,  90395,\n",
            "          16530,  62112,  99859,  49828, 101526,   7948,  68536, 109607,  99773,\n",
            "          34187,   3837,  49187, 104318,  49828, 101073,  39374,  99508,   3837,\n",
            "          28641,  13343,  63109,  64272,   3837,  44793,  36987,  56568, 100672,\n",
            "          14053, 100672, 108610,  99590, 101438,  81264,  37474,  36556, 112862,\n",
            "          44793,  36987,  20412,   3837,  20412,  75758,  75061, 105700, 101255,\n",
            "          39426,  44793,  36987,  99235, 103856,   3837,  43288,  99393,  63109,\n",
            "         115670, 100363,   3837,  56568,  99518, 100698,  49828,   9370,  11319,\n",
            "         102413,  40820, 100660,   3837, 104335,  99314, 100003,  75758,    198,\n",
            "         102903, 119516],\n",
            "        [101978,  39907, 101098,   5373, 105521, 100136,  65278,  62945,   8997,\n",
            "           9909,  63703,   7552, 106693,  47764, 101071,    198, 102749,  24300,\n",
            "         101924,     55,  43268,  30440, 100347,  99632,  18493, 111464,  99762,\n",
            "         111684,   8997,  10904, 115669,  10958,    198,  30440, 115668, 107368,\n",
            "          33071,  17447, 115610, 101304,   5373,  99180,  35551, 100439,  57191,\n",
            "          45356,  99180,  35551, 100439,   3837, 107368,  33071, 102749,   3837,\n",
            "         109470,  33071, 100242,  99316,   3837, 109470,  33071,  63109, 101408,\n",
            "         100439,  57191,  99931, 101364, 105349,  74040, 118458,  49567,   1773,\n",
            "          90919,  99931,  89481, 101364, 105349,  74040,    693,   9011, 118458,\n",
            "           3837,  38176, 101304, 100043,  24300,  33108, 100916,  24300, 112103,\n",
            "         101320,   9370, 101924, 112506,   5373, 118439,  31838, 106806, 115669,\n",
            "           3837, 100674, 106122,   3837, 100140,  87267,  57218, 107272,  99727,\n",
            "          64643, 101573,  99463, 101063,   8997,  10904, 105262,  57218, 115653,\n",
            "         105262, 144083,   9909,  14777,   7552, 105262,  39165,  38342, 100347,\n",
            "         112103, 102529]], device='cuda:0')}\u001b[0m\n",
            "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n",
            "{'loss': 3.9629, 'grad_norm': 2.307509422302246, 'learning_rate': 0.0, 'epoch': 0.0}\n",
            "{'loss': 3.903, 'grad_norm': 3.3345603942871094, 'learning_rate': 4.2857142857142856e-05, 'epoch': 0.01}\n",
            "{'loss': 3.7584, 'grad_norm': 2.427250385284424, 'learning_rate': 9.047619047619048e-05, 'epoch': 0.02}\n",
            "{'loss': 3.723, 'grad_norm': 2.1498303413391113, 'learning_rate': 0.0001380952380952381, 'epoch': 0.04}\n",
            "{'loss': 3.7738, 'grad_norm': 2.8149001598358154, 'learning_rate': 0.00018571428571428572, 'epoch': 0.05}\n",
            "{'loss': 3.6566, 'grad_norm': 2.896618604660034, 'learning_rate': 0.00019823455233291298, 'epoch': 0.06}\n",
            "  6% 50/835 [00:32<08:21,  1.56it/s]\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 10.14it/s]\u001b[A\n",
            "                                    \n",
            "\u001b[A{'eval_loss': 2.7351744174957275, 'eval_accuracy': 0.44173228346456694, 'eval_runtime': 0.7676, 'eval_samples_per_second': 13.027, 'eval_steps_per_second': 5.211, 'epoch': 0.06}\n",
            "  6% 50/835 [00:33<08:21,  1.56it/s]\n",
            "100% 4/4 [00:00<00:00,  7.40it/s]\u001b[A\n",
            "{'loss': 3.7049, 'grad_norm': 2.8689637184143066, 'learning_rate': 0.0001957124842370744, 'epoch': 0.07}\n",
            "{'loss': 3.6017, 'grad_norm': 2.6304149627685547, 'learning_rate': 0.00019319041614123582, 'epoch': 0.08}\n",
            "{'loss': 3.5201, 'grad_norm': 2.467722177505493, 'learning_rate': 0.00019066834804539723, 'epoch': 0.1}\n",
            "{'loss': 3.6276, 'grad_norm': 2.5441834926605225, 'learning_rate': 0.00018814627994955864, 'epoch': 0.11}\n",
            "{'loss': 3.6864, 'grad_norm': 2.874833583831787, 'learning_rate': 0.00018562421185372004, 'epoch': 0.12}\n",
            " 12% 100/835 [01:06<08:19,  1.47it/s]\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00,  8.87it/s]\u001b[A\n",
            " 75% 3/4 [00:00<00:00,  6.38it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 2.6705069541931152, 'eval_accuracy': 0.44881889763779526, 'eval_runtime': 0.7806, 'eval_samples_per_second': 12.81, 'eval_steps_per_second': 5.124, 'epoch': 0.12}\n",
            " 12% 100/835 [01:07<08:19,  1.47it/s]\n",
            "100% 4/4 [00:00<00:00,  6.92it/s]\u001b[A\n",
            "{'loss': 3.3939, 'grad_norm': 2.308056116104126, 'learning_rate': 0.00018310214375788145, 'epoch': 0.13}\n",
            "{'loss': 3.3739, 'grad_norm': 2.368966579437256, 'learning_rate': 0.00018058007566204288, 'epoch': 0.14}\n",
            "{'loss': 3.4998, 'grad_norm': 2.5430943965911865, 'learning_rate': 0.0001780580075662043, 'epoch': 0.16}\n",
            "{'loss': 3.6211, 'grad_norm': 2.639029026031494, 'learning_rate': 0.0001755359394703657, 'epoch': 0.17}\n",
            "{'loss': 3.5712, 'grad_norm': 2.607236623764038, 'learning_rate': 0.0001730138713745271, 'epoch': 0.18}\n",
            " 18% 150/835 [01:42<07:53,  1.45it/s]\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00,  8.96it/s]\u001b[A\n",
            " 75% 3/4 [00:00<00:00,  6.33it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 2.5972087383270264, 'eval_accuracy': 0.452755905511811, 'eval_runtime': 0.7887, 'eval_samples_per_second': 12.68, 'eval_steps_per_second': 5.072, 'epoch': 0.18}\n",
            " 18% 150/835 [01:43<07:53,  1.45it/s]\n",
            "100% 4/4 [00:00<00:00,  6.81it/s]\u001b[A\n",
            "{'loss': 3.6909, 'grad_norm': 2.727120876312256, 'learning_rate': 0.0001704918032786885, 'epoch': 0.19}\n",
            "{'loss': 3.6313, 'grad_norm': 2.1698296070098877, 'learning_rate': 0.00016796973518284995, 'epoch': 0.2}\n",
            "{'loss': 3.4376, 'grad_norm': 2.119312286376953, 'learning_rate': 0.00016544766708701135, 'epoch': 0.22}\n",
            "{'loss': 3.5555, 'grad_norm': 2.1044600009918213, 'learning_rate': 0.00016292559899117276, 'epoch': 0.23}\n",
            "{'loss': 3.5608, 'grad_norm': 2.769282817840576, 'learning_rate': 0.00016040353089533417, 'epoch': 0.24}\n",
            " 24% 200/835 [02:18<07:10,  1.47it/s]\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00,  9.12it/s]\u001b[A\n",
            " 75% 3/4 [00:00<00:00,  6.42it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 2.5574159622192383, 'eval_accuracy': 0.46062992125984253, 'eval_runtime': 0.7721, 'eval_samples_per_second': 12.952, 'eval_steps_per_second': 5.181, 'epoch': 0.24}\n",
            " 24% 200/835 [02:18<07:10,  1.47it/s]\n",
            "100% 4/4 [00:00<00:00,  6.95it/s]\u001b[A\n",
            "{'loss': 3.4795, 'grad_norm': 2.7023062705993652, 'learning_rate': 0.00015788146279949557, 'epoch': 0.25}\n",
            "{'loss': 3.5178, 'grad_norm': 2.6308305263519287, 'learning_rate': 0.000155359394703657, 'epoch': 0.26}\n",
            "{'loss': 3.4352, 'grad_norm': 2.303471565246582, 'learning_rate': 0.00015283732660781841, 'epoch': 0.28}\n",
            "{'loss': 3.5208, 'grad_norm': 2.7633140087127686, 'learning_rate': 0.00015031525851197982, 'epoch': 0.29}\n",
            "{'loss': 3.4248, 'grad_norm': 2.5350182056427, 'learning_rate': 0.00014779319041614123, 'epoch': 0.3}\n",
            " 30% 250/835 [02:53<06:41,  1.46it/s]\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00,  9.02it/s]\u001b[A\n",
            " 75% 3/4 [00:00<00:00,  6.37it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 2.562204599380493, 'eval_accuracy': 0.46141732283464565, 'eval_runtime': 0.7872, 'eval_samples_per_second': 12.703, 'eval_steps_per_second': 5.081, 'epoch': 0.3}\n",
            " 30% 250/835 [02:54<06:41,  1.46it/s]\n",
            "100% 4/4 [00:00<00:00,  6.88it/s]\u001b[A\n",
            "{'loss': 3.4849, 'grad_norm': 2.6576671600341797, 'learning_rate': 0.00014527112232030266, 'epoch': 0.31}\n",
            "{'loss': 3.4369, 'grad_norm': 2.3650314807891846, 'learning_rate': 0.00014274905422446407, 'epoch': 0.32}\n",
            "{'loss': 3.6057, 'grad_norm': 2.943925380706787, 'learning_rate': 0.00014022698612862548, 'epoch': 0.34}\n",
            "{'loss': 3.3082, 'grad_norm': 1.7580506801605225, 'learning_rate': 0.00013770491803278688, 'epoch': 0.35}\n",
            "{'loss': 3.5331, 'grad_norm': 2.447274923324585, 'learning_rate': 0.0001351828499369483, 'epoch': 0.36}\n",
            " 36% 300/835 [03:28<06:07,  1.46it/s]\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00,  8.70it/s]\u001b[A\n",
            " 75% 3/4 [00:00<00:00,  6.27it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 2.5440268516540527, 'eval_accuracy': 0.46614173228346456, 'eval_runtime': 0.7873, 'eval_samples_per_second': 12.701, 'eval_steps_per_second': 5.08, 'epoch': 0.36}\n",
            " 36% 300/835 [03:29<06:07,  1.46it/s]\n",
            "100% 4/4 [00:00<00:00,  6.85it/s]\u001b[A\n",
            "{'loss': 3.3211, 'grad_norm': 2.6158547401428223, 'learning_rate': 0.00013266078184110972, 'epoch': 0.37}\n",
            "{'loss': 3.4321, 'grad_norm': 2.523353338241577, 'learning_rate': 0.00013013871374527113, 'epoch': 0.38}\n",
            "{'loss': 3.3689, 'grad_norm': 2.4135725498199463, 'learning_rate': 0.00012761664564943254, 'epoch': 0.4}\n",
            "{'loss': 3.3942, 'grad_norm': 2.864351272583008, 'learning_rate': 0.00012509457755359394, 'epoch': 0.41}\n",
            "{'loss': 3.3169, 'grad_norm': 2.273530960083008, 'learning_rate': 0.00012257250945775535, 'epoch': 0.42}\n",
            " 42% 350/835 [04:04<05:35,  1.45it/s]\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00,  9.01it/s]\u001b[A\n",
            " 75% 3/4 [00:00<00:00,  6.34it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 2.533759593963623, 'eval_accuracy': 0.4653543307086614, 'eval_runtime': 0.7846, 'eval_samples_per_second': 12.746, 'eval_steps_per_second': 5.098, 'epoch': 0.42}\n",
            " 42% 350/835 [04:05<05:35,  1.45it/s]\n",
            "100% 4/4 [00:00<00:00,  6.92it/s]\u001b[A\n",
            "{'loss': 3.4655, 'grad_norm': 2.3946588039398193, 'learning_rate': 0.00012005044136191679, 'epoch': 0.43}\n",
            "{'loss': 3.4984, 'grad_norm': 2.8360893726348877, 'learning_rate': 0.00011752837326607819, 'epoch': 0.44}\n",
            "{'loss': 3.5153, 'grad_norm': 2.3031864166259766, 'learning_rate': 0.0001150063051702396, 'epoch': 0.46}\n",
            "{'loss': 3.3858, 'grad_norm': 2.773873805999756, 'learning_rate': 0.000112484237074401, 'epoch': 0.47}\n",
            "{'loss': 3.3959, 'grad_norm': 2.8558263778686523, 'learning_rate': 0.00010996216897856241, 'epoch': 0.48}\n",
            " 48% 400/835 [04:40<04:57,  1.46it/s]\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00,  9.12it/s]\u001b[A\n",
            " 75% 3/4 [00:00<00:00,  6.38it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 2.5385921001434326, 'eval_accuracy': 0.4622047244094488, 'eval_runtime': 0.7876, 'eval_samples_per_second': 12.697, 'eval_steps_per_second': 5.079, 'epoch': 0.48}\n",
            " 48% 400/835 [04:40<04:57,  1.46it/s]\n",
            "100% 4/4 [00:00<00:00,  6.95it/s]\u001b[A\n",
            "{'loss': 3.6041, 'grad_norm': 2.352956533432007, 'learning_rate': 0.00010744010088272385, 'epoch': 0.49}\n",
            "{'loss': 3.3722, 'grad_norm': 2.703444004058838, 'learning_rate': 0.00010491803278688525, 'epoch': 0.5}\n",
            "{'loss': 3.3949, 'grad_norm': 2.264125108718872, 'learning_rate': 0.00010239596469104666, 'epoch': 0.51}\n",
            "{'loss': 3.4202, 'grad_norm': 2.19531512260437, 'learning_rate': 9.987389659520807e-05, 'epoch': 0.53}\n",
            "{'loss': 3.2727, 'grad_norm': 2.351027011871338, 'learning_rate': 9.735182849936949e-05, 'epoch': 0.54}\n",
            " 54% 450/835 [05:15<04:22,  1.47it/s]\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00,  8.73it/s]\u001b[A\n",
            " 75% 3/4 [00:00<00:00,  6.25it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 2.4989068508148193, 'eval_accuracy': 0.47716535433070867, 'eval_runtime': 0.7922, 'eval_samples_per_second': 12.622, 'eval_steps_per_second': 5.049, 'epoch': 0.54}\n",
            " 54% 450/835 [05:16<04:22,  1.47it/s]\n",
            "100% 4/4 [00:00<00:00,  6.81it/s]\u001b[A\n",
            "{'loss': 3.4108, 'grad_norm': 2.4148216247558594, 'learning_rate': 9.48297604035309e-05, 'epoch': 0.55}\n",
            "{'loss': 3.4815, 'grad_norm': 2.5943140983581543, 'learning_rate': 9.230769230769232e-05, 'epoch': 0.56}\n",
            "{'loss': 3.4672, 'grad_norm': 2.1420505046844482, 'learning_rate': 8.978562421185372e-05, 'epoch': 0.57}\n",
            "{'loss': 3.4528, 'grad_norm': 2.576503038406372, 'learning_rate': 8.726355611601513e-05, 'epoch': 0.59}\n",
            "{'loss': 3.3459, 'grad_norm': 2.361889123916626, 'learning_rate': 8.474148802017655e-05, 'epoch': 0.6}\n",
            " 60% 500/835 [05:51<03:48,  1.46it/s]\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00,  8.76it/s]\u001b[A\n",
            " 75% 3/4 [00:00<00:00,  6.37it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 2.4997334480285645, 'eval_accuracy': 0.47874015748031495, 'eval_runtime': 0.7837, 'eval_samples_per_second': 12.76, 'eval_steps_per_second': 5.104, 'epoch': 0.6}\n",
            " 60% 500/835 [05:51<03:48,  1.46it/s]\n",
            "100% 4/4 [00:00<00:00,  6.92it/s]\u001b[A\n",
            "{'loss': 3.3068, 'grad_norm': 2.390651226043701, 'learning_rate': 8.221941992433796e-05, 'epoch': 0.61}\n",
            "{'loss': 3.3755, 'grad_norm': 2.5370099544525146, 'learning_rate': 7.969735182849938e-05, 'epoch': 0.62}\n",
            "{'loss': 3.4658, 'grad_norm': 2.542015314102173, 'learning_rate': 7.717528373266079e-05, 'epoch': 0.63}\n",
            "{'loss': 3.2959, 'grad_norm': 2.4547111988067627, 'learning_rate': 7.465321563682219e-05, 'epoch': 0.65}\n",
            "{'loss': 3.4468, 'grad_norm': 2.408689022064209, 'learning_rate': 7.213114754098361e-05, 'epoch': 0.66}\n",
            " 66% 550/835 [06:26<03:14,  1.47it/s]\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00,  8.94it/s]\u001b[A\n",
            " 75% 3/4 [00:00<00:00,  6.39it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 2.4989898204803467, 'eval_accuracy': 0.4779527559055118, 'eval_runtime': 0.7783, 'eval_samples_per_second': 12.849, 'eval_steps_per_second': 5.139, 'epoch': 0.66}\n",
            " 66% 550/835 [06:27<03:14,  1.47it/s]\n",
            "100% 4/4 [00:00<00:00,  6.96it/s]\u001b[A\n",
            "{'loss': 3.1591, 'grad_norm': 2.5848259925842285, 'learning_rate': 6.960907944514502e-05, 'epoch': 0.67}\n",
            "{'loss': 3.4368, 'grad_norm': 2.1327261924743652, 'learning_rate': 6.708701134930644e-05, 'epoch': 0.68}\n",
            "{'loss': 3.4411, 'grad_norm': 2.3217968940734863, 'learning_rate': 6.456494325346785e-05, 'epoch': 0.69}\n",
            "{'loss': 3.4886, 'grad_norm': 3.216081380844116, 'learning_rate': 6.204287515762925e-05, 'epoch': 0.71}\n",
            "{'loss': 3.4963, 'grad_norm': 2.7136316299438477, 'learning_rate': 5.9520807061790674e-05, 'epoch': 0.72}\n",
            " 72% 600/835 [07:01<02:40,  1.46it/s]\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00,  8.85it/s]\u001b[A\n",
            " 75% 3/4 [00:00<00:00,  6.39it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 2.4923858642578125, 'eval_accuracy': 0.4779527559055118, 'eval_runtime': 0.7816, 'eval_samples_per_second': 12.795, 'eval_steps_per_second': 5.118, 'epoch': 0.72}\n",
            " 72% 600/835 [07:02<02:40,  1.46it/s]\n",
            "100% 4/4 [00:00<00:00,  6.90it/s]\u001b[A\n",
            "{'loss': 3.5263, 'grad_norm': 2.6838583946228027, 'learning_rate': 5.699873896595208e-05, 'epoch': 0.73}\n",
            "{'loss': 3.3174, 'grad_norm': 2.2095165252685547, 'learning_rate': 5.44766708701135e-05, 'epoch': 0.74}\n",
            "{'loss': 3.4728, 'grad_norm': 2.328592538833618, 'learning_rate': 5.195460277427491e-05, 'epoch': 0.75}\n",
            "{'loss': 3.2982, 'grad_norm': 2.787980318069458, 'learning_rate': 4.943253467843632e-05, 'epoch': 0.77}\n",
            "{'loss': 3.411, 'grad_norm': 2.8429861068725586, 'learning_rate': 4.6910466582597736e-05, 'epoch': 0.78}\n",
            " 78% 650/835 [07:37<02:06,  1.47it/s]\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00,  8.85it/s]\u001b[A\n",
            " 75% 3/4 [00:00<00:00,  6.38it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 2.4835474491119385, 'eval_accuracy': 0.48503937007874015, 'eval_runtime': 0.783, 'eval_samples_per_second': 12.772, 'eval_steps_per_second': 5.109, 'epoch': 0.78}\n",
            " 78% 650/835 [07:37<02:06,  1.47it/s]\n",
            "100% 4/4 [00:00<00:00,  6.88it/s]\u001b[A\n",
            "{'loss': 3.3666, 'grad_norm': 2.7145302295684814, 'learning_rate': 4.438839848675915e-05, 'epoch': 0.79}\n",
            "{'loss': 3.4323, 'grad_norm': 2.752943754196167, 'learning_rate': 4.186633039092056e-05, 'epoch': 0.8}\n",
            "{'loss': 3.4251, 'grad_norm': 2.5353586673736572, 'learning_rate': 3.934426229508197e-05, 'epoch': 0.81}\n",
            "{'loss': 3.2957, 'grad_norm': 2.2450873851776123, 'learning_rate': 3.6822194199243384e-05, 'epoch': 0.83}\n",
            "{'loss': 3.2647, 'grad_norm': 2.559056520462036, 'learning_rate': 3.43001261034048e-05, 'epoch': 0.84}\n",
            " 84% 700/835 [08:12<01:31,  1.47it/s]\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00,  8.70it/s]\u001b[A\n",
            " 75% 3/4 [00:00<00:00,  6.27it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 2.479353189468384, 'eval_accuracy': 0.4811023622047244, 'eval_runtime': 0.7884, 'eval_samples_per_second': 12.684, 'eval_steps_per_second': 5.074, 'epoch': 0.84}\n",
            " 84% 700/835 [08:13<01:31,  1.47it/s]\n",
            "100% 4/4 [00:00<00:00,  6.86it/s]\u001b[A\n",
            "{'loss': 3.2691, 'grad_norm': 2.4938418865203857, 'learning_rate': 3.177805800756621e-05, 'epoch': 0.85}\n",
            "{'loss': 3.2805, 'grad_norm': 2.694549560546875, 'learning_rate': 2.9255989911727615e-05, 'epoch': 0.86}\n",
            "{'loss': 3.513, 'grad_norm': 2.5211453437805176, 'learning_rate': 2.673392181588903e-05, 'epoch': 0.87}\n",
            "{'loss': 3.3445, 'grad_norm': 2.898395538330078, 'learning_rate': 2.4211853720050443e-05, 'epoch': 0.89}\n",
            "{'loss': 3.2841, 'grad_norm': 2.2579994201660156, 'learning_rate': 2.1689785624211856e-05, 'epoch': 0.9}\n",
            " 90% 750/835 [08:47<00:58,  1.46it/s]\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00,  8.67it/s]\u001b[A\n",
            " 75% 3/4 [00:00<00:00,  6.26it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 2.4697299003601074, 'eval_accuracy': 0.4818897637795276, 'eval_runtime': 0.7881, 'eval_samples_per_second': 12.689, 'eval_steps_per_second': 5.076, 'epoch': 0.9}\n",
            " 90% 750/835 [08:48<00:58,  1.46it/s]\n",
            "100% 4/4 [00:00<00:00,  6.86it/s]\u001b[A\n",
            "{'loss': 3.2871, 'grad_norm': 2.521257162094116, 'learning_rate': 1.9167717528373267e-05, 'epoch': 0.91}\n",
            "{'loss': 3.3879, 'grad_norm': 2.2946019172668457, 'learning_rate': 1.664564943253468e-05, 'epoch': 0.92}\n",
            "{'loss': 3.5086, 'grad_norm': 2.6849164962768555, 'learning_rate': 1.4123581336696092e-05, 'epoch': 0.93}\n",
            "{'loss': 3.3259, 'grad_norm': 2.7747225761413574, 'learning_rate': 1.1601513240857503e-05, 'epoch': 0.95}\n",
            "{'loss': 3.4947, 'grad_norm': 3.0991575717926025, 'learning_rate': 9.079445145018916e-06, 'epoch': 0.96}\n",
            " 96% 800/835 [09:23<00:24,  1.46it/s]\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00,  9.00it/s]\u001b[A\n",
            " 75% 3/4 [00:00<00:00,  6.39it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 2.466604709625244, 'eval_accuracy': 0.48503937007874015, 'eval_runtime': 0.773, 'eval_samples_per_second': 12.936, 'eval_steps_per_second': 5.174, 'epoch': 0.96}\n",
            " 96% 800/835 [09:24<00:24,  1.46it/s]\n",
            "100% 4/4 [00:00<00:00,  6.94it/s]\u001b[A\n",
            "{'loss': 3.3449, 'grad_norm': 2.8133883476257324, 'learning_rate': 6.557377049180328e-06, 'epoch': 0.97}\n",
            "{'loss': 3.3525, 'grad_norm': 2.1483287811279297, 'learning_rate': 4.0353089533417404e-06, 'epoch': 0.98}\n",
            "{'loss': 3.4039, 'grad_norm': 2.5217809677124023, 'learning_rate': 1.5132408575031527e-06, 'epoch': 0.99}\n",
            "{'train_runtime': 588.4919, 'train_samples_per_second': 4.253, 'train_steps_per_second': 1.419, 'train_loss': 3.455795690399444, 'epoch': 1.0}\n",
            "100% 835/835 [09:48<00:00,  1.42it/s]\n",
            "***** train metrics *****\n",
            "  epoch                    =        1.0\n",
            "  total_flos               =   648615GF\n",
            "  train_loss               =     3.4558\n",
            "  train_runtime            = 0:09:48.49\n",
            "  train_samples            =       2503\n",
            "  train_samples_per_second =      4.253\n",
            "  train_steps_per_second   =      1.419\n",
            "\u001b[32m2025-10-24 09:45:21.735\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m736\u001b[0m - \u001b[34m\u001b[1mTraining metrics: {'train_runtime': 588.4919, 'train_samples_per_second': 4.253, 'train_steps_per_second': 1.419, 'total_flos': 696445387505664.0, 'train_loss': 3.455795690399444, 'epoch': 1.0, 'train_samples': 2503}\u001b[0m\n",
            "\u001b[32m2025-10-24 09:45:21.735\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m737\u001b[0m - \u001b[1mSaving model checkpoint to outputs-pt-v1\u001b[0m\n",
            "\u001b[32m2025-10-24 09:45:22.423\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m745\u001b[0m - \u001b[1m*** Evaluate ***\u001b[0m\n",
            "100% 4/4 [00:00<00:00,  7.09it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        1.0\n",
            "  eval_accuracy           =     0.4819\n",
            "  eval_loss               =     2.4671\n",
            "  eval_runtime            = 0:00:00.77\n",
            "  eval_samples            =         10\n",
            "  eval_samples_per_second =     12.901\n",
            "  eval_steps_per_second   =       5.16\n",
            "  perplexity              =    11.7879\n",
            "\u001b[32m2025-10-24 09:45:23.205\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m758\u001b[0m - \u001b[34m\u001b[1mEval metrics: {'eval_loss': 2.4670698642730713, 'eval_accuracy': 0.4818897637795276, 'eval_runtime': 0.7751, 'eval_samples_per_second': 12.901, 'eval_steps_per_second': 5.16, 'epoch': 1.0, 'eval_samples': 10, 'perplexity': 11.787856180176526}\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!python pretraining.py \\\n",
        "    --model_name_or_path Qwen/Qwen2.5-0.5B \\\n",
        "    --train_file_dir ./data/pretrain \\\n",
        "    --validation_file_dir ./data/pretrain \\\n",
        "    --per_device_train_batch_size 3 \\\n",
        "    --per_device_eval_batch_size 3 \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --use_peft True \\\n",
        "    --seed 42 \\\n",
        "    --bf16 \\\n",
        "    --max_train_samples 20000 \\\n",
        "    --max_eval_samples 10 \\\n",
        "    --num_train_epochs 1 \\\n",
        "    --learning_rate 2e-4 \\\n",
        "    --warmup_ratio 0.05 \\\n",
        "    --weight_decay 0.01 \\\n",
        "    --logging_strategy steps \\\n",
        "    --logging_steps 10 \\\n",
        "    --eval_steps 50 \\\n",
        "    --eval_strategy steps \\\n",
        "    --save_steps 50 \\\n",
        "    --save_strategy steps \\\n",
        "    --save_total_limit 3 \\\n",
        "    --gradient_accumulation_steps 1 \\\n",
        "    --preprocessing_num_workers 1 \\\n",
        "    --block_size 128 \\\n",
        "    --group_by_length True \\\n",
        "    --output_dir outputs-pt-v1 \\\n",
        "    --overwrite_output_dir \\\n",
        "    --ddp_timeout 30000 \\\n",
        "    --logging_first_step True \\\n",
        "    --target_modules all \\\n",
        "    --lora_rank 8 \\\n",
        "    --lora_alpha 16 \\\n",
        "    --lora_dropout 0.05 \\\n",
        "    --torch_dtype bfloat16 \\\n",
        "    --device_map auto \\\n",
        "    --report_to tensorboard \\\n",
        "    --ddp_find_unused_parameters False \\\n",
        "    --gradient_checkpointing True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LvdpRwYiYTjS",
        "outputId": "efab5573-fd09-4d29-d62a-7ba2cade0d05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 22M\n",
            "-rw-r--r-- 1 root root  930 Oct 24 09:45 adapter_config.json\n",
            "-rw-r--r-- 1 root root  17M Oct 24 09:45 adapter_model.safetensors\n",
            "-rw-r--r-- 1 root root  605 Oct 24 09:45 added_tokens.json\n",
            "-rw-r--r-- 1 root root  470 Oct 24 09:45 all_results.json\n",
            "-rw-r--r-- 1 root root 2.4K Oct 24 09:45 chat_template.jinja\n",
            "drwxr-xr-x 2 root root 4.0K Oct 24 09:44 \u001b[0m\u001b[01;34mcheckpoint-750\u001b[0m/\n",
            "drwxr-xr-x 2 root root 4.0K Oct 24 09:44 \u001b[01;34mcheckpoint-800\u001b[0m/\n",
            "drwxr-xr-x 2 root root 4.0K Oct 24 09:45 \u001b[01;34mcheckpoint-835\u001b[0m/\n",
            "-rw-r--r-- 1 root root  262 Oct 24 09:45 eval_results.json\n",
            "-rw-r--r-- 1 root root 1.6M Oct 24 09:45 merges.txt\n",
            "-rw-r--r-- 1 root root 5.1K Oct 24 09:45 README.md\n",
            "drwxr-xr-x 3 root root 4.0K Oct 24 09:35 \u001b[01;34mruns\u001b[0m/\n",
            "-rw-r--r-- 1 root root  616 Oct 24 09:45 special_tokens_map.json\n",
            "-rw-r--r-- 1 root root 4.7K Oct 24 09:45 tokenizer_config.json\n",
            "-rw-r--r-- 1 root root  20K Oct 24 09:45 trainer_state.json\n",
            "-rw-r--r-- 1 root root  228 Oct 24 09:45 train_results.json\n",
            "-rw-r--r-- 1 root root 3.3M Oct 24 09:45 vocab.json\n"
          ]
        }
      ],
      "source": [
        "%ls -lh outputs-pt-v1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bEYXfpxbYTjT"
      },
      "source": [
        "模型训练结果：\n",
        "- 使用lora训练模型，则保存的lora权重是`adapter_model.safetensors`, lora配置文件是`adapter_config.json`，合并到base model的方法见`merge_peft_adapter.py`\n",
        "- 日志保存在`output_dir/runs`目录下，可以使用tensorboard查看，启动tensorboard方式如下：`tensorboard --logdir output_dir/runs --host 0.0.0.0 --port 8009`"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tensorboard --logdir output_dir/runs --host 0.0.0.0 --port 8009"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "id": "MxApNOu5nqew",
        "outputId": "ce38cd96-760c-43b8-d07c-ba11a530f9fe"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (ipython-input-2167933048.py, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-2167933048.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    tensorboard --logdir output_dir/runs --host 0.0.0.0 --port 8009\u001b[0m\n\u001b[0m                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UkSv6kHgnq8I"
      },
      "source": [
        "模型训练结果：\n",
        "- 使用lora训练模型，则保存的lora权重是`adapter_model.safetensors`, lora配置文件是`adapter_config.json`，合并到base model的方法见`merge_peft_adapter.py`\n",
        "- 日志保存在`output_dir/runs`目录下，可以使用tensorboard查看，启动tensorboard方式如下：`tensorboard --logdir output_dir/runs --host 0.0.0.0 --port 8009`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "ve1J3hkkYTjT"
      },
      "source": [
        "lora模型权重合并到base model，合并后的模型保存在`--output_dir`目录下，合并方法如下："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g0UzDZdVYTjT"
      },
      "outputs": [],
      "source": [
        "!python merge_peft_adapter.py \\\n",
        "    --base_model Qwen/Qwen2.5-0.5B --lora_model outputs-pt-v1 --output_dir merged-pt/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IrkMVI8aYTjT"
      },
      "outputs": [],
      "source": [
        "%ls -lh merged-pt/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h8xZBDtZYTjT"
      },
      "outputs": [],
      "source": [
        "%cat merged-pt/config.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yj6QWIxrYTjT"
      },
      "source": [
        "Stage1 增量预训练完成。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-06-15T13:56:17.081153Z",
          "start_time": "2023-06-15T13:56:17.032821Z"
        },
        "id": "jOBIgT1XYTjT"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "B92XHZI_YTjT"
      },
      "source": [
        "# Stage 2: Supervised FineTuning\n",
        "\n",
        "第二阶段：SFT(Supervised Fine-tuning)有监督微调，构造指令微调数据集，在预训练模型基础上做指令精调，以对齐指令意图，并注入领域知识\n",
        "\n",
        "| Stage 2: Supervised Fine-tuning | [supervised_finetuning.py](https://github.com/shibing624/MedicalGPT/blob/main/supervised_finetuning.py) | [run_sft.sh](https://github.com/shibing624/MedicalGPT/blob/main/run_sft.sh)  |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "jEkpCyZeYTjT"
      },
      "source": [
        "#### 说明：\n",
        "以下 notebook/colab 代码为了快速验证训练代码可用，我们使用了小size的生成模型和小样本数据集，实际使用时，需要使用更大的模型和数据集，以获得更好的效果。\n",
        "\n",
        "1. 生成模型：使用的是Qwen/Qwen2.5-0.5B 或者 Stage1得到的预训练模型\n",
        "2. 数据集：SFT阶段使用的是使用的是Belle的1千条抽样数据，位于`data/finetune`文件夹"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "nVNwqX5qYTjT"
      },
      "source": [
        "## Stage2 咱们开始吧\n",
        "\n",
        "训练步骤如下：\n",
        "\n",
        "1. 确认训练集\n",
        "2. 执行训练脚本\n",
        "\n",
        "训练脚本的执行逻辑如下：\n",
        "1. 导入依赖包\n",
        "2. 设置参数\n",
        "3. 定义各函数并加载训练集\n",
        "4. 加载模型和tokenizer\n",
        "5. 开始训练并评估\n",
        "6. 查看训练结果"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-06-15T13:58:38.966506Z",
          "start_time": "2023-06-15T13:58:38.778132Z"
        },
        "id": "UCKis3k0YTjT"
      },
      "outputs": [],
      "source": [
        "%ls ./data/finetune"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ftfOWkDDYTjT"
      },
      "outputs": [],
      "source": [
        "!python supervised_finetuning.py \\\n",
        "    --model_name_or_path merged-pt \\\n",
        "    --train_file_dir ./data/finetune \\\n",
        "    --validation_file_dir ./data/finetune \\\n",
        "    --per_device_train_batch_size 4 \\\n",
        "    --per_device_eval_batch_size 4 \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --use_peft True \\\n",
        "    --bf16 \\\n",
        "    --max_train_samples 1000 \\\n",
        "    --max_eval_samples 10 \\\n",
        "    --num_train_epochs 1 \\\n",
        "    --learning_rate 2e-5 \\\n",
        "    --warmup_ratio 0.05 \\\n",
        "    --weight_decay 0.05 \\\n",
        "    --logging_strategy steps \\\n",
        "    --logging_steps 10 \\\n",
        "    --eval_steps 50 \\\n",
        "    --eval_strategy steps \\\n",
        "    --save_steps 500 \\\n",
        "    --save_strategy steps \\\n",
        "    --save_total_limit 3 \\\n",
        "    --gradient_accumulation_steps 1 \\\n",
        "    --preprocessing_num_workers 1 \\\n",
        "    --output_dir outputs-sft-v1 \\\n",
        "    --overwrite_output_dir \\\n",
        "    --ddp_timeout 30000 \\\n",
        "    --logging_first_step True \\\n",
        "    --target_modules all \\\n",
        "    --lora_rank 8 \\\n",
        "    --lora_alpha 16 \\\n",
        "    --lora_dropout 0.05 \\\n",
        "    --torch_dtype bfloat16 \\\n",
        "    --device_map auto \\\n",
        "    --report_to tensorboard \\\n",
        "    --ddp_find_unused_parameters False \\\n",
        "    --gradient_checkpointing True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q8Z825-wYTjT"
      },
      "outputs": [],
      "source": [
        "%ls -lh outputs-sft-v1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "GOqJ43McYTjT"
      },
      "source": [
        "模型训练结果：\n",
        "- 使用lora训练模型，则保存的lora权重是`adapter_model.safetensors`, lora配置文件是`adapter_config.json`，合并到base model的方法见`merge_peft_adapter.py`\n",
        "- 日志保存在`output_dir/runs`目录下，可以使用tensorboard查看，启动tensorboard方式如下：`tensorboard --logdir output_dir/runs --host 0.0.0.0 --port 8009`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "kQeOVh5aYTjT"
      },
      "source": [
        "lora模型权重合并到base model，合并后的模型保存在`--output_dir`目录下，合并方法如下："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jnARSuJ2YTjU"
      },
      "outputs": [],
      "source": [
        "!python merge_peft_adapter.py \\\n",
        "    --base_model merged-pt --lora_model outputs-sft-v1 --output_dir ./merged-sft"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fnSb8GbmYTjU"
      },
      "outputs": [],
      "source": [
        "%ls -lh merged-sft/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQo4OMC1YTjU"
      },
      "outputs": [],
      "source": [
        "%cat merged-sft/config.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "IyyMRr7TYTjU"
      },
      "source": [
        "Stage2 SFT训练完成。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-06-15T14:07:40.752635Z",
          "start_time": "2023-06-15T14:07:40.731186Z"
        },
        "id": "AJfXWRbDYTjU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "INnm4wKZYTjU"
      },
      "source": [
        "# Stage 3: DPO(Direct Preference Optimization)\n",
        "\n",
        "第三阶段：DPO(Direct Preference Optimization)直接偏好优化，DPO通过直接优化语言模型来实现对其行为的精确控制，而无需使用复杂的强化学习，也可以有效学习到人类偏好，DPO相较于RLHF更容易实现且易于训练，效果更好\n",
        "\n",
        "| Stage 3: Direct Preference Optimization        |  [dpo_training.py](https://github.com/shibing624/MedicalGPT/blob/main/dpo_training.py) | [run_dpo.sh](https://github.com/shibing624/MedicalGPT/blob/main/run_dpo.sh)    |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "ln32430WYTjU"
      },
      "source": [
        "#### 说明：\n",
        "以下 notebook/colab 代码为了快速验证训练代码可用，我们使用了小size的生成模型和小样本数据集，实际使用时，需要使用更大的模型和数据集，以获得更好的效果。\n",
        "\n",
        "1. 生成模型：使用的是`Qwen/Qwen2.5-0.5B` 或者 Stage2得到的SFT模型\n",
        "2. 数据集：DPO阶段使用的是医疗reward数据，抽样了500条，位于`data/reward`文件夹"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "3YYmN2MbYTjU"
      },
      "source": [
        "## Stage3 咱们开始吧\n",
        "\n",
        "训练步骤如下：\n",
        "\n",
        "1. 确认训练集\n",
        "2. 执行训练脚本\n",
        "\n",
        "训练脚本的执行逻辑如下：\n",
        "1. 导入依赖包\n",
        "2. 设置参数\n",
        "3. 定义各函数并加载训练集\n",
        "4. 加载模型和tokenizer\n",
        "5. 开始训练并评估\n",
        "6. 查看训练结果"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aj0j8SJXYTjU"
      },
      "outputs": [],
      "source": [
        "%ls ./data/reward/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8uPs_Yt2YTjU"
      },
      "outputs": [],
      "source": [
        "!python dpo_training.py \\\n",
        "    --model_name_or_path ./merged-sft \\\n",
        "    --template_name qwen \\\n",
        "    --train_file_dir ./data/reward \\\n",
        "    --validation_file_dir ./data/reward \\\n",
        "    --per_device_train_batch_size 3 \\\n",
        "    --per_device_eval_batch_size 1 \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --use_peft True \\\n",
        "    --max_train_samples 1000 \\\n",
        "    --max_eval_samples 500 \\\n",
        "    --max_steps 100 \\\n",
        "    --eval_steps 10 \\\n",
        "    --save_steps 50 \\\n",
        "    --max_source_length 256 \\\n",
        "    --max_target_length 256 \\\n",
        "    --output_dir outputs-dpo-v1 \\\n",
        "    --target_modules all \\\n",
        "    --lora_rank 8 \\\n",
        "    --lora_alpha 16 \\\n",
        "    --lora_dropout 0.05 \\\n",
        "    --torch_dtype bfloat16 \\\n",
        "    --bf16 True \\\n",
        "    --fp16 False \\\n",
        "    --device_map auto \\\n",
        "    --report_to tensorboard \\\n",
        "    --remove_unused_columns False \\\n",
        "    --gradient_checkpointing True \\\n",
        "    --cache_dir ./cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QhrSNw61YTjU"
      },
      "outputs": [],
      "source": [
        "%ls -lh outputs-dpo-v1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "7BcGCq5kYTjU"
      },
      "source": [
        "模型训练结果：\n",
        "- 使用lora训练模型，则保存的lora权重是`adapter_model.safetensors`, lora配置文件是`adapter_config.json`，合并到base model的方法见`merge_peft_adapter.py`\n",
        "- 日志保存在`output_dir/runs`目录下，可以使用tensorboard查看，启动tensorboard方式如下：`tensorboard --logdir output_dir/runs --host 0.0.0.0 --port 8009`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "amuuxSRuYTjU"
      },
      "source": [
        "lora模型权重合并到base model，合并后的模型保存在`--output_dir`目录下，合并方法如下："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0gvsKPg8YTjU"
      },
      "outputs": [],
      "source": [
        "!python merge_peft_adapter.py \\\n",
        "    --base_model merged-sft --lora_model outputs-dpo-v1 --output_dir merged-dpo/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zb_4jvHkYTjb"
      },
      "outputs": [],
      "source": [
        "%ls -lh merged-dpo/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ifzR8y1VYTjc"
      },
      "outputs": [],
      "source": [
        "%cat merged-dpo/config.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "dotOafw6YTjc"
      },
      "source": [
        "Stage3 偏好建模第一次训练完成。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "BnP0aPNDYTjc"
      },
      "source": [
        "**至此一个完整的训练流程演示完成。**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-06-26T12:34:29.658428Z",
          "start_time": "2023-06-26T12:34:29.620609Z"
        },
        "id": "N53D6QzrYTjc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "QHjsEYReYTjc"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-06-26T12:35:00.864463Z",
          "start_time": "2023-06-26T12:34:47.802087Z"
        },
        "id": "cfeeIKMYYTjc"
      },
      "outputs": [],
      "source": [
        "!python inference.py --base_model merged-dpo\n",
        "# 或在shell中运行\n",
        "# python inference.py --base_model merged-dpo --interactive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "TnUX_OumYTjc"
      },
      "source": [
        "Input:介绍下南京\n",
        "Response:  南京市位于江苏省西南部，是全国首批历史文化名城、国家中心城市和自由贸易试验区。\n",
        "\n",
        "完。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i7XECZvTYTjc"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "f34eed0bebedfc4b6ee51ced43d2c030fe3b92f13c149d072205ca200a67b1ec"
      }
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}